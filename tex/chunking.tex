\subsection{Chunking}
\label{sec:chunking}
\subsubsection{Fixed-Size Chunking}

\subsubsection{Sliding Window Chunking}
The second chunking technique we explore is what we call sliding window chunking, a technique originally presented by Manber in order to find similarities between different files \cite{Manber}. As opposed to partitioning files and streams of data into fixed-size chunks, which are inflexible to file and packet content boundaries, this method allows us to adapt chunk sizes based on the actual contents of the file or packet stream. This is done by choosing a fixed-size window which we slide across the entire contents of a web page, and fingerprinting each region until some number of low-order bits of the fingerprint are all 0. Once this occurs, we have found a breakpoint and the chunk boundary is set to the end of this special window. 

We compute our sliding window chunks based on the equations and parameters presented in \cite{spring} and combine them with the enhancements used in the LBFS content-based breakpoint chunking scheme \cite{lbfs}. Thus, not only do we specify a window size $\beta$ and breakpoint fingerprint value with $\gamma$ zeros in the low-order bits, we also specify a minimum and maximum chunk size. 

The purpose of chosing this scheme over the fixed-size chunking scheme is rather straight-forward. Since the chunks are chosen based on content rather than on position in the web page, minor changes will not affect surrounding chunks. In contrast, any minor change to a web page will probably shift a large number fixed-size chunks by some amount causing changes to a large number of fingerprints, which in turn leads to an overall decrease in redundancy detection. Unlike Manber, and Spring and Wetherall, who use this chunking scheme to fingerprint every possible region in a file or packet and then choose a specific subset of these fingerprints to find redundant data \cite{manber,spring}, we use the same approach as LBFS, i.e. to use these scheme merely for finer-grained chunking of the data and determine redundancies using additional hashing or fingerprinting once the chunks have been found. We discuss Rabin fingerprinting for the purposes of finding redundant data chunks in Section \ref{sec:fingerprinting}. 


