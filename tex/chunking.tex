\subsection{Chunking}
\label{sec:chunking}
\subsubsection{Fix-Sized Chunking}
The first way of partitioning the data that we explore use fix-sized chunks. If the size of the chunk is set to be \textit{n} bytes, then the first chunk is produced by taking the first \textit{n}
bytes from the stream. The second is constructed from the second portion of \textit{n} bytes and so on, until the end of
the stream is reached. If there is not a sufficient number of bytes to construct the last chunk, it is padded.

\subsubsection{Sliding Window Chunking}
This is a technique originally presented by Manber in order to find similarities between different files \cite{manber}. As opposed to partitioning files and streams of data into fix-sized chunks, this method allows us to adapt chunk sizes based on the actual contents of the file or packet stream. This is done by choosing a fix-sized window which we slide across the entire contents of a web page, and fingerprinting each region until some number of low-order bits of the fingerprint are all 0. Once this occurs, we have found a breakpoint and the chunk boundary is set to the end of this special window. 

We compute our sliding window chunks based on the equations and parameters presented in \cite{spring} and combine them with the enhancements used in the LBFS content-based breakpoint chunking scheme \cite{lbfs}. Thus, not only do we specify a window size $\beta$ and breakpoint fingerprint value with $\gamma$ zeros in the low-order bits, but we also specify a minimum and maximum chunk size. 

The purpose of choosing this scheme over the fix-sized chunking scheme is rather straight-forward. Since the chunks are chosen based on content rather than on position in the web page, minor changes will not affect surrounding chunks. In contrast, any minor change to a web page will probably shift a large number fix-sized chunks by some amount causing changes to a large number of fingerprints, which in turn leads to an overall decrease in redundancy detection. Unlike Manber, and Spring and Wetherall, who use this chunking scheme to fingerprint every possible region in a file or packet and then choose a specific subset of these fingerprints to find redundant data \cite{manber,spring}, we use the same approach as LBFS, \textit{i.e.} to use these scheme merely for finer-grained chunking of the data and determine redundancies using additional hashing or fingerprinting once the chunks have been found.


