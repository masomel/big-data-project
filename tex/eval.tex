\section{Experimental Evaluation}
\label{sec:eval}

\subsection{Obtainining Web Data}
We obtained results through a process of collecting offline data, and modifying our simulator to output information about the data being processed. 
Mainly, we observed how changes in parameters affected the miss rate as well as the number of bytes transferred between the proxy and mobile device.

In order to run our experiments, we first collected offline data. 
Over the course of four days, we issued telnet GET requests to various webpages (both desktop and mobile versions) in the morning, afternoon and evening. 
The frequency with which we made these GET requests were for the purpose of reflecting browsing patterns, and it would give us information about the change in the content of a webpage over the course of a day and over the course of multiple days. 
We stored each response in a different file and then processed the data to obtain the byte stream version of the html pages. 
Using this byte stream, we ran several experiments that gave us insight into data redundancy within webpages.

\subsection{Mobile vs. Desktop Browser Content}
Figure 6 shows the distinctions between mobile web content and desktop web content. 
Many web servers today structure their webpages differently depending on the user-agent they're serving to increase the speed with which the webpages load, to provide better service with respect to UI and various other reasons. 
Therefore, mobile pages are inherently different from desktop browsers and thereby require their own analysis. 
Figure 6 shows that the mobile version of cnn.com is only about a fifth of the size of the desktop version. 
The bytes transferred for the unchunked protocol shows that the size of the webpage remains relatively constant, and that the entire webpage has to be reloaded from the server for each request since the content is no longer "fresh". 
The bytes tranferred with the chunked protocol shows that the amount of redundancy that is eliminated in both mobile and desktop websites is proportional to the size of the web page. 
It also provides insight into exactly where our protocol performs well, and where the overhead of the protocol takes away from the benefits achieved from chunking. 
We see that on the first visit, the amount of bytes that needs to be transferred is almost twice the size of the actual content. 
This inefficiency comes from the fact that we're using chunk size of ten bytes. 
During the first visit to cnn.com, when there is no base copy of the webpage, the fingerprints representing the entire webpage need to be sent back and forth creating an inefficiency. 
However, once there is a base copy in the cache, the overhead decreases substantially. 
We can see from the graph that by the 12th visit, we are only transferring half the number of bytes as we would need to reload the entire webpage. 

\subsection{Effects of Chunk Size}
The use of chunk size of 10 bytes means that each redundant chunk saves 6 bytes because of the 4 bytes of fingerprint needed to represent that chunk. 
This led us to explore different chunksizes to find the ideal chunk size that takes into consideration the tradeoff between having a low cache miss rate and having a fingerprint map to a bigger chunk.  We obtained data in Figure 7 through visits to cnn once a day for four days. Day 1 is not shown since the cache is empty and so 100\% the contents needs to be transferred. This graph shows the relationship between percent of web content that is needed (based on cache miss rate) and chunk size.

It is clear from this graph that if we use smaller chunk sizes, the percent of content that needs to be sent decreases. 
The steeper line for chunk 5 when compared to chunk 45 shows that as the number of visits increase, the overlap of smaller chunk sizes increases faster.
However, it means that each fingerprint maps to a smaller chunk and so more fingerprints are needed to represent the small amount of data that needs to be transferred as opposed to larger chunk sizes where proportionally fewer fingerprints are needed to represent a larger amount of data. 

Figure 8 takes into account the effects of the extra bytes that are transferred to account for the fingerprints that are transferred to represent redndant chunks. In this graph we can see that as the chunk size increased, the miss rate also increased as expected, but the bytes transferred actually decreased. 
This is because if the chunk size is small it gets expensive for the mobile device to communicate which chunks it needs. 
Analysis of the trends indicate that the ideal chunk size depends on the size of content that needs to be transferred rather than the percentage of content that needs to be transferred.

\subsection{Bandwidth Reduction while Web Browsing.}
The next two graphs show what happens to bandwidth when we simulate "mobile browsing". 
The data was gathered by visiting cnn.com, nytimes.com and economist.com in an alternating basis three times a day for four days. This graph shows that the if the 'base' content of each webpage is in the cache, then less than 20\% of the content is generally new. The first three requests show that there is some, but not a lot of redundancy between webpages.
Figure 9 shows that on the the first visit, the cache is empty and 100\% of the traffic needs to be transferred. 
For the second website, almost all of it needs to be transferred (~90\%) because of lack of overlap with the first website. 
On the third, the proportion decreases further but a majority of the page still needs to be transferred. 
After this point, we have the base content for all three websites in our cache and only the differences need to be transferred from the proxy, so the proportion of content that needs to be transferred stays below 20\% after the sixth url request. 
Figure 9 calculates the proportion of content that needs to be transferred based on the cache miss rate but does not take into account the additional bytes that have to be transferred due to fingerprints.

Figure 10 shows the actual bandwidth savings obtained from chunking. This plot takes into account the fingerprint bytes and shows the total number of bytes that were transferred for the 32 requests.
We assume that no response is identical to a previous response. 
This means that without chunking, the full webpage has to be reloaded for each request, leading to the linearly increasing number of bytes we see in the graph. 
With chunking however, we see that past the first few requests in which the effects of the overhead are heavy, the number of total bytes transferred rises gradually, and the gap between the bytes transferred for chunking and not chunking grows with the number of requests.

\begin{figure}[h] 
\centering \includegraphics[width=\columnwidth]{images/desktopmobile.png}
\caption{Desktop vs Mobile Browser page differences.}
\end{figure}
\begin{figure}[h] 
\centering \includegraphics[width=\columnwidth]{images/chunksize.png}
\caption{Effects of Chunk Size on portion of content needed}
\end{figure}
\begin{figure}[h] 
\centering \includegraphics[width=\columnwidth]{images/chunksize2.png}
\caption{Effects of chunk size on Bytes Transferred. .}
\end{figure}
\begin{figure}[h] 
\centering \includegraphics[width=\columnwidth]{images/browsing.png}
\caption{Mobile Web Browsing. }
\end{figure}
\begin{figure}[h] 
\centering \includegraphics[width=\columnwidth]{images/cumulbrowsing.png}
\caption{Cumulative bytes transferred during browsing. }
\end{figure}
\begin{figure}[h]
\centering \includegraphics[width=\columnwidth]{images/caches.pdf}
\caption{Missrate of Different Caches}
\end{figure}
